{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq, TrainerCallback, T5Config\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchsummary import summary\n",
    "\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9257d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "                                 question  \\\n",
      "0                What is (are) Glaucoma ?   \n",
      "1                  What causes Glaucoma ?   \n",
      "2     What are the symptoms of Glaucoma ?   \n",
      "3  What are the treatments for Glaucoma ?   \n",
      "4                What is (are) Glaucoma ?   \n",
      "\n",
      "                                              answer           source  \\\n",
      "0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
      "1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n",
      "2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n",
      "3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n",
      "4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
      "\n",
      "  focus_area  \n",
      "0   Glaucoma  \n",
      "1   Glaucoma  \n",
      "2   Glaucoma  \n",
      "3   Glaucoma  \n",
      "4   Glaucoma  \n",
      "Null Value Data:\n",
      "question       0\n",
      "answer         5\n",
      "source         0\n",
      "focus_area    14\n",
      "dtype: int64\n",
      "Number of duplicate rows: 48\n",
      "Table Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16364 entries, 0 to 16363\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  16364 non-null  object\n",
      " 1   answer    16359 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 255.8+ KB\n",
      "None\n",
      "Null Value Data After Cleaning:\n",
      "question    0\n",
      "answer      0\n",
      "dtype: int64\n",
      "Unique questions: 13843\n",
      "Unique answers: 13851\n",
      "Final Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13857 entries, 0 to 13856\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  13857 non-null  object\n",
      " 1   answer    13857 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 216.6+ KB\n",
      "Final Data Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is glaucoma ?</td>\n",
       "      <td>glaucoma is a group of diseases that can damag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what causes glaucoma ?</td>\n",
       "      <td>nearly 2.7 million people have glaucoma, a lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what are the symptoms of glaucoma ?</td>\n",
       "      <td>symptoms of glaucoma glaucoma can develop in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what are the treatments for glaucoma ?</td>\n",
       "      <td>although open-angle glaucoma cannot be cured, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who is at risk for glaucoma? ?</td>\n",
       "      <td>anyone can develop glaucoma. some people are a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question  \\\n",
       "0                      what is glaucoma ?   \n",
       "1                  what causes glaucoma ?   \n",
       "2     what are the symptoms of glaucoma ?   \n",
       "3  what are the treatments for glaucoma ?   \n",
       "4          who is at risk for glaucoma? ?   \n",
       "\n",
       "                                              answer  \n",
       "0  glaucoma is a group of diseases that can damag...  \n",
       "1  nearly 2.7 million people have glaucoma, a lea...  \n",
       "2  symptoms of glaucoma glaucoma can develop in o...  \n",
       "3  although open-angle glaucoma cannot be cured, ...  \n",
       "4  anyone can develop glaucoma. some people are a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from a CSV file\n",
    "df = pd.read_csv('../data/medquad.csv')\n",
    "\n",
    "# Display a sample of the data to understand its structure\n",
    "print(\"Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for null values in the dataset\n",
    "print(\"Null Value Data:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Define a list of common question words to filter relevant questions\n",
    "question_words = ['what', 'who', 'why', 'when', 'where', 'how', 'is', 'are', 'does', 'do', 'can', 'will', 'shall']\n",
    "\n",
    "# Convert all questions to lowercase for consistent filtering\n",
    "df['question'] = df['question'].str.lower()\n",
    "\n",
    "# Filter rows where the question starts with one of the question words\n",
    "df = df[df['question'].str.split().str[0].isin(question_words)]\n",
    "\n",
    "# Reset the index after filtering\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Check for duplicate rows in the dataset\n",
    "duplicates = df.duplicated()\n",
    "print(f\"Number of duplicate rows: {duplicates.sum()}\")\n",
    "\n",
    "# Remove duplicate rows to ensure data uniqueness\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Reset the index after removing duplicates\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop unused columns ('source' and 'focus_area') to simplify the dataset\n",
    "df = df.drop(columns=['source', 'focus_area'])\n",
    "\n",
    "# Display dataset information (columns, data types, and non-null counts)\n",
    "print(\"Table Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Remove duplicate rows based on the 'question' and 'answer' columns\n",
    "df = df.drop_duplicates(subset='question', keep='first').reset_index(drop=True)\n",
    "df = df.drop_duplicates(subset='answer', keep='first').reset_index(drop=True)\n",
    "\n",
    "# Drop rows with null values in the 'question' or 'answer' columns\n",
    "df = df.dropna(subset=['question', 'answer']).reset_index(drop=True)\n",
    "\n",
    "# Fill any remaining null values with empty strings and convert to string type\n",
    "df['question'] = df['question'].fillna('').astype(str)\n",
    "df['answer'] = df['answer'].fillna('').astype(str)\n",
    "\n",
    "# Define a function to clean text by removing parentheses and extra spaces\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\(.*?\\)\", \"\", text)  # Remove text within parentheses\n",
    "    text = re.sub(r'\\s+', ' ', text.strip().lower())  # Normalize spaces and convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'question' and 'answer' columns\n",
    "df['question'] = df['question'].apply(clean_text)\n",
    "df['answer'] = df['answer'].apply(clean_text)\n",
    "\n",
    "# Further clean the text by ensuring lowercase, stripping whitespace, and normalizing spaces\n",
    "df['question'] = df['question'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "df['answer'] = df['answer'].str.lower().str.strip().apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "# Check for null values again after cleaning\n",
    "print(\"Null Value Data After Cleaning:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the number of unique questions and answers in the dataset\n",
    "print(f\"Unique questions: {df['question'].nunique()}\")\n",
    "print(f\"Unique answers: {df['answer'].nunique()}\")\n",
    "\n",
    "# Display dataset information and a sample of the cleaned data\n",
    "print(\"Final Dataset Info:\")\n",
    "df.info()\n",
    "print(\"Final Data Sample:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305b309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Model Summary:\n",
      "==================================================\n",
      "Layer Type                    Count     Parameters     \n",
      "=======================================================\n",
      "T5ForConditionalGeneration    1         222,882,048    \n",
      "Embedding                     3         24,653,568     \n",
      "T5Stack                       2         247,534,848    \n",
      "ModuleList                    26        396,455,424    \n",
      "T5Block                       24        198,227,712    \n",
      "T5LayerSelfAttention          24        56,642,304     \n",
      "T5Attention                   36        84,935,424     \n",
      "Linear                        193       222,833,664    \n",
      "T5LayerNorm                   62        47,616         \n",
      "Dropout                       86        0              \n",
      "T5LayerFF                     24        113,264,640    \n",
      "T5DenseActDense               24        113,246,208    \n",
      "ReLU                          24        0              \n",
      "T5LayerCrossAttention         12        28,320,768     \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeb1ef455ae430aaed8bca78ae92f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/11778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e6c63c945440cb85181896bd3a831c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1591' max='4419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1591/4419 56:31 < 1:40:36, 0.47 it/s, Epoch 1.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.134500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.522700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.393100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.107700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 189\u001b[0m\n\u001b[1;32m    178\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m    179\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    180\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m eval_pred: compute_metrics(eval_pred, tokenizer),\n\u001b[1;32m    186\u001b[0m )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Save the trained model and tokenizer\u001b[39;00m\n\u001b[1;32m    192\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./t5_chatbot_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/transformers/trainer.py:3791\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3789\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3791\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2553\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2553\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/mastering/project_mml/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the model name and load the T5 configuration\n",
    "model_name = \"t5-base\"\n",
    "config = T5Config.from_pretrained(model_name)\n",
    "\n",
    "# Customize the configuration\n",
    "config.dropout_rate = 0.1  # Set dropout rate to 0.1 for regularization\n",
    "config.feed_forward_proj = \"gelu\"  # Use GELU activation for the feed-forward layers\n",
    "\n",
    "# Load the pre-trained T5 model with the customized configuration\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Load the tokenizer for the T5 model\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Explicitly resize the token embeddings to match the tokenizer's vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Print a detailed summary of the model architecture\n",
    "print(\"\\nDetailed Model Summary:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def summarize_model_by_type(model):\n",
    "    \"\"\"\n",
    "    Summarizes the model by counting the number of layers and parameters for each layer type.\n",
    "    \"\"\"\n",
    "    layer_summary = defaultdict(int)  # Counts the number of layers by type\n",
    "    param_summary = defaultdict(int)  # Counts the number of parameters by layer type\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        layer_type = type(module).__name__  # Get the type of the current module\n",
    "        layer_summary[layer_type] += 1  # Increment the count for this layer type\n",
    "        param_summary[layer_type] += sum(p.numel() for p in module.parameters())  # Sum parameters\n",
    "\n",
    "    # Print the summary table\n",
    "    print(f\"{'Layer Type':<30}{'Count':<10}{'Parameters':<15}\")\n",
    "    print(\"=\" * 55)\n",
    "    for layer_type, count in layer_summary.items():\n",
    "        print(f\"{layer_type:<30}{count:<10}{param_summary[layer_type]:<15,}\")\n",
    "\n",
    "summarize_model_by_type(model)\n",
    "\n",
    "# Define a preprocessing function for the seq2seq task (optimized for speed)\n",
    "def preprocess_function(batch):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataset by tokenizing the inputs and targets.\n",
    "    Optimized for speed by using fast tokenization and avoiding unnecessary conversions.\n",
    "    \"\"\"\n",
    "    # Format the inputs and targets\n",
    "    inputs = [f\"answer the following question: {q}\" for q in batch['question']]\n",
    "    targets = [str(a) for a in batch['answer']]\n",
    "\n",
    "    # Tokenize the inputs and targets in one call each, return as lists (not tensors)\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "    # Replace padding token IDs with -100 for the loss function to ignore them\n",
    "    labels_ids = [\n",
    "        [(lid if lid != tokenizer.pad_token_id else -100) for lid in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "    model_inputs[\"labels\"] = labels_ids\n",
    "    return model_inputs\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42)\n",
    "\n",
    "# Convert the pandas DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Preprocess the training and validation datasets\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=16,  # Process in batches of 16\n",
    "    remove_columns=train_dataset.column_names,  # Remove original columns\n",
    "    num_proc=4,  # Use 4 processes for parallel processing\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=16,  # Process in batches of 16\n",
    "    remove_columns=val_dataset.column_names,  # Remove original columns\n",
    "    num_proc=4,  # Use 4 processes for parallel processing\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_eval=True,\n",
    "    save_total_limit=1,  # Guarda solo el último checkpoint para ahorrar espacio\n",
    "    learning_rate=5e-4,  # Puedes subirlo a 1e-3 para convergencia más rápida, pero cuidado con la estabilidad\n",
    "    num_train_epochs=1,  # Solo 1 época para pruebas rápidas\n",
    "    per_device_train_batch_size=16,  # Aumenta el batch size si tu RAM lo permite\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.05,  # Menor warmup para acelerar el inicio\n",
    "    weight_decay=0.01,  # Menor regularización para acelerar el aprendizaje\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    no_cuda=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,  # Menos logs para menos overhead\n",
    "    gradient_accumulation_steps=1,  # Sin acumulación para pasos más rápidos\n",
    "    max_grad_norm=1.0,\n",
    "    dataloader_num_workers=2,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=True,\n",
    "    label_smoothing_factor=0.05,\n",
    ")\n",
    "\n",
    "# Initialize the data collator for seq2seq tasks\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding='longest',  # Pad sequences to the longest in the batch\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"\n",
    "    Computes exact match, BLEU, and ROUGE-L metrics for evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Normalize text for comparison\n",
    "    decoded_preds = [text.strip().lower() for text in decoded_preds]\n",
    "    decoded_labels = [text.strip().lower() for text in decoded_labels]\n",
    "\n",
    "    # Compute exact match\n",
    "    exact_match = np.mean([p == l for p, l in zip(decoded_preds, decoded_labels)])\n",
    "\n",
    "    # Load BLEU and ROUGE metrics\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_score = bleu_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=[[label] for label in decoded_labels]\n",
    "    )[\"bleu\"]\n",
    "\n",
    "    # Compute ROUGE-L score\n",
    "    rouge_score = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels\n",
    "    )[\"rougeL\"]\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-L\": rouge_score,\n",
    "    }\n",
    "\n",
    "# Initialize the Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "trainer.save_model(\"./t5_chatbot_model\")\n",
    "tokenizer.save_pretrained(\"./t5_chatbot_tokenizer\")\n",
    "\n",
    "# Save the model's state dictionary\n",
    "model_path = \"./t5_chatbot_model.h5\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save the training log history\n",
    "log_history = trainer.state.log_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b0836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
